#!/usr/bin/env python3
"""
å¥–åŠ±æ¨¡å‹è®­ç»ƒè„šæœ¬ - PEFTä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨æ–¹æ³•:
python reward_model_train_no_wandb.py \
    --base_model Qwen/Qwen2-0.5B-Instruct \
    --dataset_path D:/Github/trl/datasets/HPD_EDP \
    --output_dir ./Qwen2-0.5B-2-Reward-LoRA \
    --max_length 2048 \
    --batch_size 4 \
    --learning_rate 5e-5 \
    --num_train_epochs 3
"""

import os
import math
import torch
import argparse
import numpy as np
import time
import psutil
import logging
from datetime import datetime
from typing import Dict, List, Optional, Union
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
    PeftConfig
)
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from sklearn.metrics import accuracy_score
import warnings

warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_log.txt'),
        logging.StreamHandler()
    ]
)


def parse_args():
    parser = argparse.ArgumentParser(description="è®­ç»ƒå¥–åŠ±æ¨¡å‹")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen2-0.5B-Instruct",
                        help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument("--dataset_path", type=str, default="D:\\Github\\trl\\datasets\\HPD_EDP",
                        help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./Qwen2-0.5B-3-Reward-LoRA",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_length", type=int, default=2048,
                        help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--batch_size", type=int, default=4,
                        help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--num_train_epochs", type=int, default=3,
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--use_8bit", action="store_true",
                        help="æ˜¯å¦ä½¿ç”¨8ä½é‡åŒ–")
    parser.add_argument("--eval_steps", type=int, default=200,
                        help="è¯„ä¼°é—´éš”æ­¥æ•°")
    parser.add_argument("--save_steps", type=int, default=200,
                        help="ä¿å­˜é—´éš”æ­¥æ•°")
    parser.add_argument("--warmup_ratio", type=float, default=0.1,
                        help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--logging_steps", type=int, default=10,
                        help="æ—¥å¿—è®°å½•é—´éš”æ­¥æ•°")
    parser.add_argument("--lora_rank", type=int, default=8,
                        help="LoRAç§©")
    parser.add_argument("--lora_alpha", type=int, default=16,
                        help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05,
                        help="LoRA dropout")
    parser.add_argument("--target_modules", type=str, default="q_proj,v_proj",
                        help="LoRAç›®æ ‡æ¨¡å—ï¼Œç”¨é€—å·åˆ†éš”")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                        help="ä»checkpointæ¢å¤è®­ç»ƒçš„è·¯å¾„")
    return parser.parse_args()


def format_conversation(messages):
    """å°†å¯¹è¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå•ä¸ªå­—ç¬¦ä¸²"""
    if isinstance(messages, list):
        conversation = ""
        for msg in messages:
            if isinstance(msg, dict) and "role" in msg and "content" in msg:
                role = msg["role"]
                content = msg["content"]
                if role == "user":
                    conversation += f"Human: {content}\n\n"
                elif role == "assistant":
                    conversation += f"Assistant: {content}\n\n"
                else:
                    conversation += f"{role}: {content}\n\n"
        return conversation.strip()
    elif isinstance(messages, str):
        return messages
    else:
        return str(messages)


def prepare_dataset(dataset, tokenizer, max_length):
    """å‡†å¤‡æ•°æ®é›†ï¼Œå°†chosenå’Œrejectedæ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥"""
    print(dataset['chosen'][0])

    def tokenize_function(examples):
        if isinstance(examples["chosen"], list) and len(examples["chosen"]) > 0:
            chosen_texts = [format_conversation(item) for item in examples["chosen"]]
        else:
            chosen_texts = [format_conversation(examples["chosen"])]

        if isinstance(examples["rejected"], list) and len(examples["rejected"]) > 0:
            rejected_texts = [format_conversation(item) for item in examples["rejected"]]
        else:
            rejected_texts = [format_conversation(examples["rejected"])]

        # æ ‡è®°åŒ–chosenæ–‡æœ¬
        chosen_tokens = tokenizer(
            chosen_texts,
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors="pt"
        )

        # æ ‡è®°åŒ–rejectedæ–‡æœ¬
        rejected_tokens = tokenizer(
            rejected_texts,
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors="pt"
        )

        # ç»„åˆæˆæ¨¡å‹æ‰€éœ€æ ¼å¼
        model_inputs = {
            "input_ids_chosen": chosen_tokens["input_ids"],
            "attention_mask_chosen": chosen_tokens["attention_mask"],
            "input_ids_rejected": rejected_tokens["input_ids"],
            "attention_mask_rejected": rejected_tokens["attention_mask"],
            "labels": torch.ones(len(chosen_texts), dtype=torch.float)
        }

        return model_inputs

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )

    return tokenized_dataset


class RewardDataCollator:
    """è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ï¼Œå¤„ç†æˆå¯¹çš„æ•°æ®"""

    def __init__(self, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __call__(self, examples):
        input_ids_chosen = torch.stack(
            [example["input_ids_chosen"] if isinstance(example["input_ids_chosen"], torch.Tensor)
             else torch.tensor(example["input_ids_chosen"]) for example in examples])
        attention_mask_chosen = torch.stack(
            [example["attention_mask_chosen"] if isinstance(example["attention_mask_chosen"], torch.Tensor)
             else torch.tensor(example["attention_mask_chosen"]) for example in examples])
        input_ids_rejected = torch.stack(
            [example["input_ids_rejected"] if isinstance(example["input_ids_rejected"], torch.Tensor)
             else torch.tensor(example["input_ids_rejected"]) for example in examples])
        attention_mask_rejected = torch.stack(
            [example["attention_mask_rejected"] if isinstance(example["attention_mask_rejected"], torch.Tensor)
             else torch.tensor(example["attention_mask_rejected"]) for example in examples])

        batch = {
            "input_ids_chosen": input_ids_chosen,
            "attention_mask_chosen": attention_mask_chosen,
            "input_ids_rejected": input_ids_rejected,
            "attention_mask_rejected": attention_mask_rejected,
            "labels": torch.ones(len(examples), dtype=torch.float)
        }

        return batch


def get_gpu_memory_usage():
    """è·å–GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        gpu_metrics = {}
        for i in range(torch.cuda.device_count()):
            gpu_metrics.update({
                f'gpu_{i}_memory_used': torch.cuda.memory_allocated(i) / (1024 ** 3),  # è½¬æ¢ä¸ºGB
                f'gpu_{i}_memory_cached': torch.cuda.memory_reserved(i) / (1024 ** 3),  # è½¬æ¢ä¸ºGB
                f'gpu_{i}_memory_free': (torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(
                    i)) / (1024 ** 3)  # è½¬æ¢ä¸ºGB
            })
        return gpu_metrics
    return {'gpu_memory': 'N/A'}


def get_cpu_memory_usage():
    """è·å–CPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    return {
        'cpu_memory_percent': psutil.virtual_memory().percent,
        'cpu_memory_used_gb': psutil.virtual_memory().used / (1024 ** 3),  # è½¬æ¢ä¸ºGB
        'cpu_memory_free_gb': psutil.virtual_memory().free / (1024 ** 3)  # è½¬æ¢ä¸ºGB
    }


def log_system_metrics():
    """è®°å½•ç³»ç»ŸæŒ‡æ ‡"""
    metrics = {
        **get_gpu_memory_usage(),
        **get_cpu_memory_usage(),
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    logging.info(f"ç³»ç»ŸæŒ‡æ ‡: {metrics}")
    return metrics


class RewardTrainer(Trainer):
    """è‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œç”¨äºå¤„ç†å¥–åŠ±æ¨¡å‹çš„æˆå¯¹æ¯”è¾ƒè®­ç»ƒ"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_time = time.time()
        self.last_metrics_log = time.time()
        self.metrics_interval = 60  # æ¯60ç§’è®°å½•ä¸€æ¬¡ç³»ç»ŸæŒ‡æ ‡

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        input_ids_chosen = inputs["input_ids_chosen"]
        attention_mask_chosen = inputs["attention_mask_chosen"]
        input_ids_rejected = inputs["input_ids_rejected"]
        attention_mask_rejected = inputs["attention_mask_rejected"]

        # è·å–chosenæ–‡æœ¬çš„åˆ†æ•°
        chosen_outputs = model(
            input_ids=input_ids_chosen,
            attention_mask=attention_mask_chosen
        )
        chosen_rewards = chosen_outputs.logits.squeeze(-1)

        # è·å–rejectedæ–‡æœ¬çš„åˆ†æ•°
        rejected_outputs = model(
            input_ids=input_ids_rejected,
            attention_mask=attention_mask_rejected
        )
        rejected_rewards = rejected_outputs.logits.squeeze(-1)

        # è®¡ç®—æŸå¤± - æˆ‘ä»¬å¸Œæœ›chosençš„åˆ†æ•°é«˜äºrejectedçš„åˆ†æ•°
        loss = -torch.nn.functional.logsigmoid(chosen_rewards - rejected_rewards).mean()

        if return_outputs:
            return loss, (chosen_outputs, rejected_outputs)
        return loss

    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        """è‡ªå®šä¹‰è¯„ä¼°æ–¹æ³•ï¼Œè®¡ç®—åå¥½å‡†ç¡®ç‡"""
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        metrics = {}
        self.model.eval()

        all_chosen_rewards = []
        all_rejected_rewards = []

        eval_dataloader = self.get_eval_dataloader(eval_dataset)

        for batch in tqdm(eval_dataloader, desc="Evaluating"):
            batch = {k: v.to(self.model.device) for k, v in batch.items()}

            with torch.no_grad():
                chosen_outputs = self.model(
                    input_ids=batch["input_ids_chosen"],
                    attention_mask=batch["attention_mask_chosen"]
                )
                chosen_rewards = chosen_outputs.logits.squeeze(-1).cpu().numpy()

                rejected_outputs = self.model(
                    input_ids=batch["input_ids_rejected"],
                    attention_mask=batch["attention_mask_rejected"]
                )
                rejected_rewards = rejected_outputs.logits.squeeze(-1).cpu().numpy()

                all_chosen_rewards.extend(chosen_rewards)
                all_rejected_rewards.extend(rejected_rewards)

        accuracy = (np.array(all_chosen_rewards) > np.array(all_rejected_rewards)).mean()
        metrics[f"{metric_key_prefix}_accuracy"] = float(accuracy)

        mean_diff = np.mean(np.array(all_chosen_rewards) - np.array(all_rejected_rewards))
        metrics[f"{metric_key_prefix}_mean_diff"] = float(mean_diff)

        metrics[f"{metric_key_prefix}_loss"] = float(self.compute_loss(
            self.model,
            {
                "input_ids_chosen": batch["input_ids_chosen"],
                "attention_mask_chosen": batch["attention_mask_chosen"],
                "input_ids_rejected": batch["input_ids_rejected"],
                "attention_mask_rejected": batch["attention_mask_rejected"]
            }
        ).item())

        return metrics


def load_datasets(dataset_path):
    """åŠ è½½æ•°æ®é›†"""
    try:
        if os.path.isdir(dataset_path):
            dataset = load_dataset(dataset_path)
        else:
            dataset = load_dataset(dataset_path)

        splits = list(dataset.keys())

        if "train" not in splits:
            raise ValueError("æ•°æ®é›†å¿…é¡»åŒ…å«trainåˆ†å‰²")

        if "validation" not in splits and "test" not in splits:
            dataset = dataset["train"].train_test_split(test_size=0.1)

        return dataset
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return None


def main():
    args = parse_args()
    start_time = time.time()

    # è®°å½•åˆå§‹ç³»ç»ŸçŠ¶æ€
    initial_metrics = log_system_metrics()
    logging.info("ğŸš€ å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
    logging.info(f"ğŸ“ åŸºç¡€æ¨¡å‹: {args.base_model}")
    logging.info(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {args.dataset_path}")
    logging.info(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    logging.info(f"ğŸ”¢ æœ€å¤§é•¿åº¦: {args.max_length}")
    logging.info(f"ğŸ’¾ æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    logging.info(f"ğŸ“ˆ å­¦ä¹ ç‡: {args.learning_rate}")
    logging.info(f"ğŸ”„ è®­ç»ƒè½®æ•°: {args.num_train_epochs}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # é…ç½®é‡åŒ–å‚æ•°
    if args.use_8bit:
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
    else:
        bnb_config = None

    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        args.base_model,
        trust_remote_code=True,
        use_fast=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(
        args.base_model,
        num_labels=1,
        torch_dtype=torch.float16,
        trust_remote_code=True,
        quantization_config=bnb_config,
        device_map="auto"
    )

    # é…ç½®LoRA
    target_modules = args.target_modules.split(",")
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        inference_mode=False,
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=target_modules,
        bias="none",
        modules_to_save=None
    )

    # å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒ
    if args.use_8bit:
        model = prepare_model_for_kbit_training(model)

    # åº”ç”¨LoRA
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # ç¡®ä¿PAD token idè®¾ç½®æ­£ç¡®
    model.config.pad_token_id = tokenizer.pad_token_id

    # åŠ è½½æ•°æ®é›†
    logging.info("\nğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    dataset = load_datasets(args.dataset_path)

    if dataset is None:
        print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return

    # å‡†å¤‡æ•°æ®é›†
    logging.info("ğŸ”„ æ­£åœ¨å‡†å¤‡æ•°æ®é›†...")
    train_dataset = prepare_dataset(dataset["train"], tokenizer, args.max_length)

    eval_dataset = None
    if "validation" in dataset:
        eval_dataset = prepare_dataset(dataset["validation"], tokenizer, args.max_length)
    elif "test" in dataset:
        eval_dataset = prepare_dataset(dataset["test"], tokenizer, args.max_length)

    # è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        eval_strategy="steps",
        eval_steps=args.eval_steps if eval_dataset else None,
        save_steps=args.save_steps,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        num_train_epochs=args.num_train_epochs,
        warmup_ratio=args.warmup_ratio,
        logging_steps=args.logging_steps,
        load_best_model_at_end=True if eval_dataset else False,
        greater_is_better=True,
        fp16=True,
        logging_dir=f"{args.output_dir}/logs",
        report_to=[],  # ä¸ä½¿ç”¨ä»»ä½•æŠ¥å‘Šå·¥å…·
        remove_unused_columns=False
    )

    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨
    data_collator = RewardDataCollator(tokenizer, args.max_length)

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = RewardTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # å¼€å§‹è®­ç»ƒ
    logging.info("ğŸš‚ å¼€å§‹è®­ç»ƒ...")
    if args.resume_from_checkpoint:
        logging.info(f"ğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ: {args.resume_from_checkpoint}")
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    else:
        trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logging.info("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    # è®¡ç®—æ€»ç”¨æ—¶
    total_time = time.time() - start_time
    hours, remainder = divmod(total_time, 3600)
    minutes, seconds = divmod(remainder, 60)

    # è®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    final_metrics = log_system_metrics()
    final_stats = {
        'total_training_time_hours': hours,
        'total_training_time_minutes': minutes,
        'total_training_time_seconds': seconds,
        'initial_metrics': initial_metrics,
        'final_metrics': final_metrics
    }

    logging.info(f"è®­ç»ƒå®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯: {final_stats}")
    logging.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()